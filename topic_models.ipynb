{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "nbpresent": {
     "id": "da6a10b5-775d-4761-8151-1b15c22d9b3a"
    }
   },
   "outputs": [],
   "source": [
    "import numpy\n",
    "import spacy\n",
    "from spacy.parts_of_speech import DET, ADP, CONJ, PUNCT, SPACE\n",
    "from gensim import corpora, models, similarities\n",
    "import time\n",
    "import datetime\n",
    "# %matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib as mpl\n",
    "sns.set_style('whitegrid')\n",
    "sns.set_context('poster')\n",
    "import logging\n",
    "logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.INFO)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbpresent": {
     "id": "4f95f373-e314-4ba0-bb63-ed1877493205"
    }
   },
   "source": [
    "# NLP "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "nbpresent": {
     "id": "c4972cbd-567f-4f56-a7f1-0cfdff151da8"
    }
   },
   "outputs": [],
   "source": [
    "from mendeley_import import iterate_db\n",
    "nlp = spacy.en.English()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "nbpresent": {
     "id": "40df90bd-ac67-4974-abb8-c7e3dd1f47cc"
    }
   },
   "outputs": [],
   "source": [
    "id_list, abstract_list, title_list, added_list = [],[],[],[]\n",
    "for id, abstract, title, added, modified, doi, arxivId, citationKey, pmid, year in iterate_db():\n",
    "\n",
    "    somePhrases = [DET, ADP, CONJ, PUNCT, SPACE]\n",
    "    if abstract:\n",
    "        tokens = nlp(abstract)\n",
    "        # make them cannonical form (singlular vs plural etc) and filter out articles etc\n",
    "        parsed_abstract = [t.lemma_ for t in tokens if t.pos not in somePhrases and not t.is_stop]\n",
    "    else:\n",
    "        parsed_abstract = []\n",
    "        continue  # skiping ones with no abstract\n",
    "\n",
    "    if title:\n",
    "        tokens = nlp(title)\n",
    "        parsed_title = [t.lemma_ for t in tokens if t.pos not in somePhrases]\n",
    "\n",
    "    # process teh time stamp: if added on 6/Feb/2015 (backup restored) take the 'modified' otherwise the 'added'\n",
    "    feb6 = datetime.date(2015,2,6)\n",
    "    current_added = datetime.date.fromtimestamp(added//1000)  # /1000 to get rid of milisecs\n",
    "    current_mod = datetime.date.fromtimestamp(modified//1000)\n",
    "    timestamp = modified if current_added == feb6 and current_mod < current_added else added\n",
    "    \n",
    "#     timestamp = added\n",
    "\n",
    "    id_list.append(id)\n",
    "    abstract_list.append(parsed_abstract)\n",
    "    title_list.append(parsed_title)\n",
    "    added_list.append(timestamp)\n",
    "\n",
    "docs = abstract_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "nbpresent": {
     "id": "15aed4ef-a7cc-4424-aaef-7f6b92c75f22"
    }
   },
   "outputs": [],
   "source": [
    "print(\"%d papers loaded\" % len(docs))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## plotting time evolution of papercounts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "nbpresent": {
     "id": "ca8b2916-04ad-4dec-89a3-11ca09a23d84"
    }
   },
   "outputs": [],
   "source": [
    "%matplotlib\n",
    "\n",
    "time_array = np.array(added_list)\n",
    "\n",
    "q,w = np.histogram(time_array, bins=72)\n",
    "plt.plot(w[1:],q, 'k')\n",
    "plt.xlim(min(time_array), max(time_array))\n",
    "plt.ylim(0,200)\n",
    "locs,xticklabels = zip(*[(datetime.datetime(_,1,1).timestamp()*1000, _) for _ in [2011,2012,2013,2014,2015,2016,2017]])\n",
    "plt.xticks(locs, xticklabels)\n",
    "plt.xlabel('Time')\n",
    "plt.ylabel('#papers')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "nbpresent": {
     "id": "a269f556-f7b5-4c53-8ec9-89c0d2c782a1"
    }
   },
   "outputs": [],
   "source": [
    "\"apply bigrams\"\n",
    "bigram = models.Phrases(docs, min_count=5)\n",
    "for idx in range(len(docs)):\n",
    "    for token in bigram[docs[idx]]:\n",
    "        if '_' in token:\n",
    "            # Token is a bigram, add to document.\n",
    "            docs[idx].append(token)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbpresent": {
     "id": "6d8d0d0e-6ac2-4c26-82f5-32d9a6ec24ec"
    }
   },
   "source": [
    "# Buliding the dictionary/corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "nbpresent": {
     "id": "d905fcef-662b-49cd-bd08-3893049353b0"
    }
   },
   "outputs": [],
   "source": [
    "# the dictionary//features\n",
    "# remove rare/comon tokens\n",
    "dictionary = corpora.Dictionary(docs)\n",
    "\n",
    "import toolz.dicttoolz as dt\n",
    "# whats the most frequent workd\n",
    "dictionary.dfs  # the #documents containing a word not work vount in total\n",
    "freq = dt.valfilter(lambda x: x>200, dictionary.dfs)\n",
    "print([(dictionary[k],v) for k,v in freq.items() if v/dictionary.num_docs> 0.25])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "nbpresent": {
     "id": "f6d375fd-5f11-4c2d-bd81-14f23a7a3274"
    }
   },
   "outputs": [],
   "source": [
    "dictionary.filter_extremes(no_below=2, no_above=0.25)\n",
    "\n",
    "## BoW transform\n",
    "corpus = [dictionary.doc2bow(doc) for doc in docs]\n",
    "corpora.MmCorpus.serialize('corpus_abstracts.mm', corpus)\n",
    "\n",
    "print(corpus)\n",
    "# tfidf = models.TfidfModel(corpus)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbpresent": {
     "id": "da943b78-58bc-40d3-9a89-261ccba2914c"
    }
   },
   "source": [
    "# LDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "nbpresent": {
     "id": "81ecf880-8ece-4fbb-aed3-e7554f6a9283"
    }
   },
   "outputs": [],
   "source": [
    "corpus = corpora.MmCorpus('corpus_abstracts.mm')\n",
    "print(corpus)\n",
    "\n",
    "# Set training parameters.\n",
    "num_topics = 15\n",
    "chunksize = 2000\n",
    "passes = 20\n",
    "iterations = 400\n",
    "eval_every = None  # Don't evaluate model perplexity, takes too much time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "nbpresent": {
     "id": "495c92b1-2ee4-4f86-82c2-7cba0f04c323"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "LDA\n",
    "\"\"\"\n",
    "# Make a index to word dictionary.\n",
    "temp = dictionary[0]  # This is only to \"load\" the dictionary.\n",
    "id2word = dictionary.id2token\n",
    "\n",
    "model = models.LdaModel(corpus=corpus, id2word=id2word, chunksize=chunksize,\n",
    "                       alpha='auto', eta='auto',\n",
    "                       iterations=iterations, num_topics=num_topics,\n",
    "                       passes=passes, eval_every=eval_every)\n",
    "\n",
    "\"eval the topics\"\n",
    "top_topics = model.top_topics(corpus, num_words=20)\n",
    "\n",
    "# Average topic coherence is the sum of topic coherences of all topics, divided by the number of topics.\n",
    "avg_topic_coherence = sum([t[1] for t in top_topics]) / num_topics\n",
    "print('Average topic coherence: %.4f.' % avg_topic_coherence)\n",
    "\n",
    "from pprint import pprint\n",
    "pprint(top_topics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "nbpresent": {
     "id": "35115166-63db-4644-8624-ca8c4fbf23f2"
    }
   },
   "outputs": [],
   "source": [
    "model.save('myLDA.pkl')\n",
    "corpus.save('myCorpus.pkl')\n",
    "dictionary.save('myDict.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "nbpresent": {
     "id": "d95fef0c-953d-4e45-9caa-7dfb6cda8a46"
    }
   },
   "outputs": [],
   "source": [
    "model = models.LdaModel.load('myLDA.pkl', mmap='r')\n",
    "corpus = corpora.MmCorpus('corpus_abstracts.mm')\n",
    "dictionary = corpora.Dictionary.load('myDict.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "nbpresent": {
     "id": "4cf8cbb4-eef1-434a-aaac-85e62271dcc1"
    }
   },
   "outputs": [],
   "source": [
    "import pyLDAvis.gensim\n",
    "pyLDAvis.enable_notebook()\n",
    "Q = pyLDAvis.gensim.prepare(model, corpus, dictionary)\n",
    "# ldatopics = model.show_topics(formatted=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "nbpresent": {
     "id": "9a3eaef4-070a-45eb-89bd-72a07e49aeea"
    }
   },
   "outputs": [],
   "source": [
    "pyLDAvis.display(Q )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Topics over time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "nbpresent": {
     "id": "e4c5ee3b-99ea-49b8-b178-83448260343c"
    }
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "using the timestamp, look at the prevalance of each topic over time\n",
    "\"\"\"\n",
    "n_bins = 10\n",
    "time_bins = np.linspace(min(added_list),max(added_list), n_bins)\n",
    "dt = time_bins[1]- time_bins[0]\n",
    "topics_over_time = np.zeros((num_topics, n_bins))\n",
    "docs_per_time = np.zeros(n_bins)\n",
    "\n",
    "docs_array = np.array(docs) # convert to array for easier indexing\n",
    "added_array = np.array(added_list)  # convert to array for easier indexing\n",
    "\n",
    "for i,t in enumerate(time_bins):\n",
    "    relevant_doc_ix = np.where(np.logical_and(added_array < t+dt, added_array>=t ))[0]\n",
    "    n_docs = len(relevant_doc_ix)\n",
    "\n",
    "    print(\"time %d: #docs: %d\" % (i, n_docs))\n",
    "\n",
    "    rel_docs = docs_array[relevant_doc_ix].tolist()\n",
    "    BoW = [dictionary.doc2bow(_) for _ in rel_docs]\n",
    "    topics_per_doc = [model.get_document_topics(_) for _ in BoW] # each entry is a tuple\n",
    "\n",
    "    for topic_prob_tuple in topics_per_doc:\n",
    "        for topic_id, prob in topic_prob_tuple:\n",
    "            topics_over_time[topic_id,i] =  topics_over_time[topic_id,i] + prob\n",
    "\n",
    "    docs_per_time[i] += n_docs\n",
    "    \n",
    "marginals_topic = topics_over_time.sum(1)/topics_over_time.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "nbpresent": {
     "id": "cf2075e9-2e32-4237-b11c-c43457f35dc0"
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "normed_topic_per_time = (topics_over_time/(docs_per_time+1)).T\n",
    "plt.figure()\n",
    "for i in range(num_topics):\n",
    "    j= i//5\n",
    "    plt.subplot(2,2,j+1)\n",
    "    plt.plot(time_bins, normed_topic_per_time[:,i])\n",
    "             \n",
    "    plt.ylim([0, 1])\n",
    "    locs,xticklabels = zip(*[(datetime.datetime(_,1,1).timestamp()*1000, _) for _ in [2011,2012,2013,2014,2015,2016,2017]])\n",
    "    plt.xticks(locs, xticklabels)\n",
    "    plt.xlabel('Time')\n",
    "    plt.ylabel('#papers/topic')\n",
    "\n",
    "plt.legend(range(5))\n",
    "# plt.matshow((topics_over_time/(docs_per_time+1)).T)\n",
    "\n",
    "             \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "nbpresent": {
     "id": "daa661f0-e5c0-48b7-8a07-91953de27e84"
    }
   },
   "outputs": [],
   "source": [
    "# plot only the top10 (marignal distr) topics\n",
    "ix_sort = np.array(list(reversed(np.argsort(marginals_topic))))  # most probable at tehe front"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "nbpresent": {
     "id": "5815542c-0185-41bb-9fc0-1beebe04e556"
    }
   },
   "outputs": [],
   "source": [
    "cmap = [plt.cm.Accent(_) for _ in np.linspace(0,1,8)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print(plt.style.available)\n",
    "plt.style.use('dark_background')\n",
    "mpl.rcParams['grid.linewidth'] = 0.3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "nbpresent": {
     "id": "443eaf25-ff6b-4e00-8d74-5d99f2f2bac6"
    }
   },
   "outputs": [],
   "source": [
    "leg = []\n",
    "for i in range(8):\n",
    "    current_topicID = ix_sort[i]\n",
    "    plt.plot(time_bins, normed_topic_per_time[:,current_topicID], color=cmap[i])\n",
    "    plt.ylim([0, 1])\n",
    "    locs,xticklabels = zip(*[(datetime.datetime(_,1,1).timestamp()*1000, _) for _ in [2011,2012,2013,2014,2015,2016,2017]])\n",
    "    plt.xticks(locs, xticklabels)\n",
    "    plt.xlabel('Time')\n",
    "    plt.ylabel('P(topic)')\n",
    "    \n",
    "    tmp_leg = \"-\".join(list(zip(*model.show_topic(current_topicID)))[0][:4])\n",
    "    leg.append(tmp_leg)\n",
    "plt.legend(leg)\n",
    "plt.ylim([0,0.6])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "normed_topic_per_time[0,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "nbpresent": {
     "id": "7473f00e-0277-408e-add6-53c48238a28a"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model.show_topic(4,topn=100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbpresent": {
     "id": "915155c4-4ebc-488c-bc77-d24f18e40553"
    }
   },
   "source": [
    "# Dynamic topic model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "nbpresent": {
     "id": "aa137638-0c82-4fc0-a61a-5707c26dd9cf"
    }
   },
   "outputs": [],
   "source": [
    "from gensim.models import ldaseqmodel\n",
    "from gensim.corpora import Dictionary, bleicorpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "nbpresent": {
     "id": "d3cffc7b-b17f-4ae1-af37-d4322f847ee4"
    }
   },
   "outputs": [],
   "source": [
    "time_slice = docs_per_time.astype('int').tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "nbpresent": {
     "id": "f20b46f3-51b2-4efd-98ae-1f53a558326e"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "ldaseq = ldaseqmodel.LdaSeqModel(corpus=corpus, id2word=dictionary, time_slice=time_slice, num_topics=5, lda_model=model)"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda env:gensim]",
   "language": "python",
   "name": "conda-env-gensim-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  },
  "toc": {
   "toc_cell": false,
   "toc_number_sections": true,
   "toc_threshold": "4",
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
